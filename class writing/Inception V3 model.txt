VGG 이후 나온 모델이며 GoogleNet이라고 불리기도 하였으나 
google에서 inception의 여러 버전 중 하나가 GoogleNet이며 Inception이라는 논문으로 발표하였다.

딥러닝은 망이 깊을수록 레이어가 넓을수록 성능이 좋지만 
현실적으로 overfeating, vanishing 등의 문제로 실제 학습이 어렵다.


overfeating 과적합 현상
학습량이 많아짐에 따라 학습데이터에 대해서는 오차가 감소하지만
실제 데이터(테스트 데이터)에 대해서 오히려 오차가 증가하는 현상

vanishing ( gradient problem ) 기울기 소실 문제
은닉층이 많은 다층 퍼셉트론에서, 은닉층을 많이 거칠수록 전달되는 오차가 크게 줄어들어
학습이 되지않는 현상

시그모이드 함수와 같은 함수는 출력 값이 1 아래여서 기울기 소멸 문제가 빠르게 일어난다.
이를 해결하기 위해 사라져가는 성질을 갖지 않는 비선형 함수 활성화함수인 Relu함수로 해결할 수 있다.

보통적인 CNN모델은 하나의 Convolution필터로 연산하지만 Inception모델에서는 작은 Conv레이어 여러 개를 한 층에서 구현하는 형태이다.

개선된 모델로 1x1Conv사용한다.
1x1연산은 특성맵을 줄이는 목적으로 사용


global average pooling
AlexNet, VGGNet 등에서는 fully connected (FC) 층들이 망의 후반부에 연결되어 있다. 
그러나 GoogLeNet은 FC 방식 대신에 global average pooling이란 방식을 사용한다. 
global average pooling은 전 층에서 산출된 특성맵들을 각각 평균낸 것을 이어서 1차원 벡터를 만들어주는 것이다. 
1차원 벡터를 만들어줘야 최종적으로 이미지 분류를 위한 softmax 층을 연결해줄 수 있기 때문이다. 
만약 전 층에서 1024장의 7 x 7의 특성맵이 생성되었다면, 1024장의 7 x 7 특성맵 각각 평균내주어 얻은 1024개의 값을 하나의 벡터로 연결해주는 것이다.


auxiliary classifier
네트워크의 깊이가 깊어지면 깊어질수록 vanishing gradient 문제를 피하기 어려워진다. 
그러니까 가중치를 훈련하는 과정에 역전파(back propagation)를 주로 활용하는데, 역전파과정에서 가중치를 업데이트하는데 
사용되는 gradient가 점점 작아져서 0이 되어버리는 것이다. 따라서 네트워크 내의 가중치들이 제대로 훈련되지 않는다. 
이 문제를 극복하기 위해서 GoogleNet에서는 네트워크 중간에 두 개의 보조 분류기(auxiliary classifier)를 달아주었다. - softmax함수 중간에 두개를 두어 사용